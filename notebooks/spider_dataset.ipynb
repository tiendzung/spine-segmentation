{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, List\n",
    "import monai\n",
    "import monai.transforms\n",
    "import SimpleITK as sitk\n",
    "import numpy as np \n",
    "import torch\n",
    "from lightning import LightningDataModule\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset, random_split\n",
    "from torchvision.transforms import transforms\n",
    "import json\n",
    "import os\n",
    "import rootutils\n",
    "\n",
    "rootutils.setup_root(search_from=\"/work/hpc/spine-segmentation/notebooks/logger_wandb.ipynb\", indicator=\"setup.py\", pythonpath=True)\n",
    "from src.data.transforms import array\n",
    "\n",
    "# always starting with vanilla dataset, like its a norm to me now\n",
    "class SpiderDataset(Dataset):\n",
    "    num_class = 15\n",
    "    def __init__(self, \n",
    "                 data = None,\n",
    "                 data_dir: str = \"\", \n",
    "                 json_path: str = \"\",\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.data = list()\n",
    "        self.data_dir = data_dir\n",
    "        if data is not None:\n",
    "            self.data = data\n",
    "        else:\n",
    "            if data_dir == \"\" or json_path == \"\":\n",
    "                raise AssertionError(\"No dataset ?\")\n",
    "            self.setup(json_path)\n",
    "    \n",
    "    def setup(self, json_path):\n",
    "        json_object = json.load(open(json_path, \"r\"))\n",
    "        keys = json_object.keys()\n",
    "        if \"training\" in keys:\n",
    "            for key in keys:\n",
    "                self.data.extend(json_object[key])\n",
    "        else:\n",
    "            try:\n",
    "                self.data.extend(json_object)\n",
    "            except:\n",
    "                raise InsertionError(\"Something wrong with json file, cannot load or do anything, at all\")\n",
    "    \n",
    "    def get_item(self, index: int):\n",
    "        output = dict()\n",
    "        output[\"image\"] = \"\"\n",
    "        output[\"label\"] = \"\"\n",
    "        if isinstance(self.data[index][\"image\"], list):\n",
    "            output[\"image\"] = [os.path.join(self.data_dir, image) for image in self.data[index][\"image\"]]\n",
    "        else:\n",
    "            # Add for debugging\n",
    "            path = os.path.join(self.data_dir, self.data[index][\"image\"])\n",
    "            output[\"image\"] = path ##Set to 4 because the model_inferr in validation_step\n",
    "        output[\"label\"] = os.path.join(self.data_dir, self.data[index][\"label\"])\n",
    "        return output\n",
    "    \n",
    "    # In case they query in list of index\n",
    "    def __getitem__(self, index):\n",
    "        output = None\n",
    "        if not isinstance(index, int):\n",
    "            output = []\n",
    "            for id in index:\n",
    "                output.append(self.get_item(id))\n",
    "        else:\n",
    "            output = self.get_item(int(index))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "\n",
    "class SpiderTransformedDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 dataset: SpiderDataset,\n",
    "                 transform: monai.transforms.Compose):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        output = None\n",
    "        \n",
    "        if not isinstance(index, int):\n",
    "            output = []\n",
    "            for id in index:\n",
    "                output.append(self.transform(self.dataset[id]))\n",
    "        else:\n",
    "            output = self.transform(self.dataset[int(index)])\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpiderDataset(data_dir = \"/data/hpc/spine/dataset/spine_nii\", json_path=\"/data/hpc/spine/jsons/test.json\")\n",
    "# dataset = SpiderDataset(data_dir = \"./data/dataset\", json_path=\"/data/hpc/spine/jsons/brats21_folds.json\")\n",
    "\n",
    "transform = monai.transforms.Compose([monai.transforms.LoadImaged(keys=[\"image\", \"label\"], image_only = False),\n",
    "                                        array.ConvertToMultiChannelBasedOnSpiderClassesdSemantic(keys=[\"label\"]),\n",
    "                                        monai.transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "                                        # Spacingd(\n",
    "                                        #     keys=[\"image\", \"label\"],\n",
    "                                        #     pixdim=(1.0, 1.0, 1.0),\n",
    "                                        #     mode=(\"bilinear\", \"nearest\"),\n",
    "                                        # ),\n",
    "                                    #   monai.transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=[\"label\"]),\n",
    "                                    #   monai.transforms.Resized(keys=[\"image\", \"label\"], spatial_size=(250, 250, 155)),\n",
    "                                        monai.transforms.ToTensord(keys=[\"image\", \"label\"]),])\n",
    "\n",
    "transformed = SpiderTransformedDataset(dataset, transform)\n",
    "data = dataset[1]\n",
    "images = transformed[1]\n",
    "print(data)\n",
    "print(images[\"image\"].size(), images[\"image\"].dtype)\n",
    "print(images[\"label\"].size(), images[\"label\"].dtype)\n",
    "print(images.keys())\n",
    "print(\"------------------\")\n",
    "print(images['image_meta_dict'])\n",
    "print(\"------------------\")\n",
    "print(images['label_meta_dict'])\n",
    "print(images[\"label\"].size())\n",
    "\n",
    "# res = 0\n",
    "# for image in transformed:\n",
    "#     res = max(res, image[\"label\"].size(0))\n",
    "\n",
    "# print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
